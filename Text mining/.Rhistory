knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(lubridate)
library(data.table)
library(ggplot2)
library(plotly)
#install.packages("gridExtra")
library(gridExtra)
#install.packages("ggpubr")
library(ggpubr)
library(tidytext)
library(wordcloud2)
library(RColorBrewer)
library(topicmodels)
#Cargo los df
df_pfizer_RT <- read.csv2("../Data/df_pfizer_RT_final.csv")
df_sputnik_RT <- read.csv2("../Data/df_sputnik_RT_final.csv")
#Creamos una función para agrupar los df por tipo de vacuna y contar el n por fecha
fn_agrupar <- function(df) {
df <- df %>% mutate(created_at = ymd_hms(created_at))
df <- df %>% mutate(created_at = floor_date(created_at, unit = "day"))
#Agrupamos los tweets por tipo de vacuna
df <- df %>% group_by(vacuna)
#Contamos los tweets por día y tipo
df <- df %>% count(created_at)
return(df)
}
#Creamos una función para tokenizar nuestros datasets.
fn_tokenizar <- function(tk) {
tk <- tk %>% unnest_tokens(word,text)
return(tk)
}
#Creamos una función para contar la frecuencia de palabras
fn_frecuencia <- function(frec) {
frec <- frec %>% count(word, sort = TRUE)
print(frec)
}
#Creo una columna nueva llamada "tipo" en los df con RT donde se va a aclarar si habla sobre la vacuna sputnikv o pfizer.
vacuna <- sample("pfizer", replace = TRUE)
df_pfizer_RT <- cbind(df_pfizer_RT, vacuna)
vacuna <- sample("sputnikv", replace = TRUE)
df_sputnik_RT <- cbind(df_sputnik_RT, vacuna)
#Uno los df de sputnik y pfizer en uno solo que se va a llamar df.
df_rt <- rbind(df_pfizer_RT, df_sputnik_RT)
#Argentina
df_argentina_rt <- df_rt %>%
filter(location %like% "argentina")
#Chile
df_chile_rt <- df_rt %>%
filter(location %like% "chile")
#Bolivia
df_bolivia_rt <- df_rt %>%
filter(location %like% "bolivia")
#Paraguay
df_paraguay_rt <- df_rt %>%
filter(location %like% "paraguay")
#Uruguay
df_uruguay_rt <- df_rt %>%
filter(location %like% "uruguay")
#Argentina
contar_arg <- fn_agrupar(df = df_argentina_rt)
#Chile
contar_chl <- fn_agrupar(df = df_chile_rt)
#Paraguay
contar_py <- fn_agrupar(df = df_paraguay_rt)
#Uruguay
contar_uy <- fn_agrupar(df = df_uruguay_rt)
#Bolivia
contar_bo <- fn_agrupar(df = df_bolivia_rt)
p_diario_arg <- ggplot(contar_arg) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "Fecha", y = "n tweets", title = "Cantidad de tweets por día y vacuna - Argentina") +
theme_minimal()
ggplotly(p_diario_arg)
##### Chile
p_diario_ch <- ggplot(contar_chl) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "Fecha",
y = "n tweets",
title = "Cantidad de tweets por día y vacuna - Chile") +
theme_minimal()
ggplotly(p_diario_ch)
p_diario_bo <- ggplot(contar_bo) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "Fecha", y = "n tweets", title = "Cantidad de tweets por día y vacuna - Bolivia") +
theme_minimal()
ggplotly(p_diario_bo)
p_diario_uy <- ggplot(contar_uy) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "Fecha", y = "n tweets", title = "Cantidad de tweets por día y vacuna - Uruguay") +
theme_minimal()
ggplotly(p_diario_uy)
p_diario_py <- ggplot(contar_py) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "Fecha", y = "n tweets", title = "Cantidad de tweets por día y vacuna - Paraguay") +
theme_minimal()
ggplotly(p_diario_py)
#Argentina con titulo simple
p_corto_arg <- ggplot(contar_arg) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "fecha", y = "n", title = "ARG") +
theme_minimal()
#Paraguay con titulo simple
p_corto_py <- ggplot(contar_py) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "fecha", y = "n", title = "PY") +
theme_minimal()
#Uruguay con título simple
p_corto_uy <- ggplot(contar_uy) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "fecha", y = "n", title = "UY") +
theme_minimal()
#Chile con título simple
p_corto_chl <- ggplot(contar_chl) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "fecha", y = "n", title = "CL") +
theme_minimal()
#Bolivia con título simple
p_corto_bo <- ggplot(contar_bo) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
labs(x = "fecha", y = "n", title = "BO") +
theme_minimal()
#Unimos los gráficos con ggarrange
p_union <- ggarrange(p_corto_arg, p_corto_py, p_corto_chl, p_corto_uy, p_corto_bo,
common.legend = TRUE,
nrow = 3, ncol = 2)
p_union
#Agregemos fechas importantes al grafico. El 24 de diciembre trajeron las primeras dosis de la sputnikv y el 29 de diciembre comenzaron con la vacunación.
fecha_dosis_sputnik_arg <- ymd("2020-12-24")
fecha_vacunacion_sputnik_arg <- ymd("2020-12-29")
p_hitos_arg <- ggplot(contar_arg) +
geom_line(aes(x = created_at, y = n, color = vacuna)) +
geom_text(aes( x=created_at, y=n, label=n), size=3, fontface="italic") +
scale_x_datetime(date_labels = "%d-%m", date_breaks = "3 day") +
labs(x = "Fecha", y = "Número de tweets", title = "Cantidad de tweets por día y vacuna - Argentina") +
geom_vline(aes(xintercept = as.POSIXct(fecha_dosis_sputnik_arg)),
color = "orange", alpha = .5)  +
annotate("text", x = as.POSIXct(fecha_dosis_sputnik_arg), y = 0,
label = "Arribo dosis SpV", size = 3) +
geom_vline(aes(xintercept = as.POSIXct(fecha_vacunacion_sputnik_arg)),
color = "orange", alpha = .5)  +
annotate("text", x = as.POSIXct(fecha_vacunacion_sputnik_arg), y = 0,
label = "Inicio vacunación SpV", size = 3) +
theme_minimal()
p_hitos_arg
ggsave("p_hitos_arg.png")
#Eliminar rt y respuestas.
df_organicos_arg <- df_argentina_rt %>%
subset(is.na(reply_to_status_id)) %>%
filter(is_retweet == FALSE)
#Nos quedamos solo con los rt
df_rtweets_arg <- df_argentina_rt %>%
filter(is_retweet==TRUE)
#Nos quedamos solo con las respuestas
df_respuestas_arg <- df_argentina_rt %>% subset(!is.na(reply_to_status_id))
#Creo un marco con la cantidad de rt, organico y respuestas
categorias_arg <- data.frame(category = c ("Organico", "Retweets", "Respuestas"),
count=c(7215, 37506, 2834))
#Calculamos la fracción que hay de cada categoria
categorias_arg$fraction = categorias_arg$count / sum(categorias_arg$count)
#Calculamos el porcentaje que hay de cada categoria
categorias_arg$percentage = categorias_arg$count / sum(categorias_arg$count) * 100
#Redondeamos el porcentaje de la columna 4 (percentage)
categorias_arg[,4] <-round(categorias_arg[,4],0)
ggplot(categorias_arg) +
geom_bar(mapping = aes(x= category, y = percentage, fill = category), stat = "identity") +
scale_fill_brewer(palette = "Accent") +
labs(x = "Categoria", y = "Porcentaje", title = "Porcentaje de tweets por categoría") +
geom_text(aes(x=category, y=percentage, label=percentage), size=3)
#df base sin rt
df_nort <- df_rt %>%
filter(is_retweet == FALSE)
#Creamos un df con datos de argentina que nos sirva para realizar luego el analisis de stop words. Pfizer
df_arg_noRT_pfizer <- df_nort %>%
filter(location %like% "argentina" & vacuna == "pfizer")
#Filtramos
count_arg_pfizer <- df_arg_noRT_pfizer %>%
select(screen_name, text, retweet_count, favorite_count, followers_count, verified) %>%
filter(retweet_count >= 100 | favorite_count >= 300) %>%
arrange(desc(retweet_count))
view(count_arg_pfizer)
#Creamos un df con datos de argentina que nos sirva para realizar luego el analisis de stop words. SputnikV
df_arg_noRT_sputnik <- df_nort %>%
filter(location %like% "argentina" & vacuna == "sputnikv")
#Filtramos
count_arg_sputnik <- df_arg_noRT_sputnik %>%
select(screen_name, text, retweet_count, favorite_count, followers_count, verified) %>%
filter(retweet_count >= 100 | favorite_count >= 300) %>%
arrange(desc(retweet_count))
view(count_arg_sputnik)
#genero un dataset que tenga todas las palabras de los tweets, 1 por fila, utilizando la función creada anteriormente "fn_tokenizar"
#Pfizer
token_pfizer_arg <- fn_tokenizar(tk = df_arg_noRT_pfizer)
#SputnikV
token_sputnik_arg <- fn_tokenizar(tk = df_arg_noRT_sputnik)
# Genero dataframe especial con stop words en español, de la librería tm
custom_stop_words <- bind_rows(stop_words,
data_frame(word = tm::stopwords("spanish"), lexicon = "custom"))
# Veo las stopwords
#unique(custom_stop_words$word)
#Realicemos tambien un listado de stop words que no aportan a nuestro analisis
palabras_sinaporte <- tibble(word = c("vacunas", "si", "informa", "a", "aa", "acá"))
#Argentina 1. sputnik, 2. pfizer
token_sputnik_arg <- token_sputnik_arg %>%
anti_join(custom_stop_words) %>%
anti_join(palabras_sinaporte)
token_pfizer_arg <- token_pfizer_arg %>%
anti_join(custom_stop_words) %>%
anti_join(palabras_sinaporte)
#Pfizer
fn_frecuencia(frec = token_pfizer_arg)
#SputnikV
fn_frecuencia(frec = token_sputnik_arg)
# Ploteamos las palabras más frecuentes
token_pfizer_arg %>%
count(word, sort = TRUE) %>%
filter(n > 200 & n < 2000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = word)) +
geom_col() +
labs(y = "cantidad", title = "Cantidad de veces que se menciona cada palabra para tweets sobre Pfizer", subtitle = "Argentina") +
xlab(NULL) +
scale_fill_brewer(palette = "Spectral") +
geom_text(aes( x=word, y=n, label=n), size=3) +
coord_flip()
# Ploteamos las palabras más frecuentes
token_sputnik_arg %>%
count(word, sort = TRUE) %>%
filter(n > 600 & n < 6000) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = word)) +
geom_col() +
labs(y = "cantidad", title = "Cantidad de veces que se menciona cada palabra para tweets sobre SputnikV", subtitle = "Argentina") +
xlab(NULL) +
scale_fill_brewer(palette = "Spectral") +
geom_text(aes( x=word, y=n, label=n), size=3) +
coord_flip()
# Word cloud
token_pfizer_arg %>%
count(word, sort=T) %>%
filter(n < 2000) %>% #eliminamos la palabra pfizer de la nube
wordcloud2(size = 1, color = 'random-light', backgroundColor = "black")
# Word cloud
token_sputnik_arg %>%
count(word, sort=T) %>%
filter(n < 6000) %>% #eliminamos la palabra sputnik de la nube
wordcloud2(size = 1, color = 'random-light', backgroundColor = "black")
#Genero la matriz término documento con la función ```cast_dtm()```
data_conteo_pfizer <- token_pfizer_arg %>% count(status_id, word, sort=TRUE)
data_dtm_pfizer <- data_conteo_pfizer %>%
cast_dtm(status_id, word, n)
data_dtm_pfizer $nrow
data_dtm_pfizer $ncol
# Con k indico la cantidad de tópicos
data_lda_pfizer <- LDA(data_dtm_pfizer, k = 4, control = list(seed = 150))
# Paso el objeto a tidy, beta son las probabilidades per-topic-per-word
data_lda_td_pfizer <- tidy(data_lda_pfizer, matrix = "beta")
# Veo los términos más frecuentes de mi topic modelling
terminos_frecuentes_pfizer <- data_lda_td_pfizer %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Ploteo mis tópicos
terminos_frecuentes_pfizer %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
labs(title = "Tópicos") +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
data_conteo_sputnik <- token_sputnik_arg %>% count(status_id, word, sort=TRUE)
data_dtm_sputnik <- data_conteo_sputnik %>%
cast_dtm(status_id, word, n)
data_dtm_sputnik $nrow
data_dtm_sputnik $ncol
# Con k indico la cantidad de tópicos
data_lda_sputnik <- LDA(data_dtm_sputnik, k = 4, control = list(seed = 150))
# Paso el objeto a tidy, beta son las probabilidades per-topic-per-word
data_lda_td_sputnik <- tidy(data_lda_sputnik, matrix = "beta")
# Veo los términos más frecuentes de mi topic modelling
terminos_frecuentes_sputnik <- data_lda_td_sputnik %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
arrange(topic, -beta)
# Ploteo mis tópicos
terminos_frecuentes_sputnik %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
labs(title = "Tópicos") +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
